{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "How to use the Radiant MLHub API to browse and download the LandCoverNet dataset\n",
    "=====\n",
    "\n",
    "This Jupyter notebook, which you may copy and adapt for any use, shows basic examples of how to use the API to download labels and source imagery for the LandCoverNet dataset. Full documentation for the API is available at [docs.mlhub.earth](http://docs.mlhub.earth).\n",
    "\n",
    "We'll show you how to set up your authorization, list collection properties, and retrieve the items (the data contained within them) from those collections.\n",
    "\n",
    "Each item in our collection is explained in json format compliant with STAC label extension definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation\n",
    "====\n",
    "Alemohammad S.H., Ballantyne A., Bromberg Gaber Y., Booth K., Nakanuku-Diggs L., & Miglarese A.H. (2020) \"LandCoverNet: A Global Land Cover Classification Training Dataset\", Version 1.0, Radiant MLHub. \\[Date Accessed\\] [https://doi.org/10.34911/rdnt.d2ce8i](https://doi.org/10.34911/rdnt.d2ce8i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication\n",
    "====\n",
    "\n",
    "Access to the Radiant MLHub API requires an access token. To get your access token, go to [dashboard.mlhub.earth](https://dashboard.mlhub.earth). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. Under **Usage**, you'll see your access token, which you will need. Do not share your access token with others: your usage may be limited and sharing your access token is a security risk.\n",
    "\n",
    "Copy the access token, and paste it in the box bellow. This header block will work for all API calls.\n",
    "\n",
    "The Collection ID for the labels of the LandCoverNet dataset can be found on the [registry page](https://registry.mlhub.earth/10.34911/rdnt.d2ce8i). We will save that to a variable which will be used during API requests.\n",
    "\n",
    "Click **Run** or press SHIFT + ENTER before moving on to run this first piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_BASE = 'http://api.radiant.earth/mlhub/v1'\n",
    "\n",
    "ACCESS_TOKEN = 'PASTE_YOUR_ACCESS_TOKEN_HERE'\n",
    "\n",
    "# these headers will be used in each request\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {ACCESS_TOKEN}',\n",
    "    'Accept':'application/json'\n",
    "}\n",
    "\n",
    "COLLECTION_ID = 'ref_landcovernet_v1_labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing Collection Properties\n",
    "====\n",
    "\n",
    "The code below will make a request to the API requesting the properties for the collection we selected. The code below prints out a few important properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: LandCoverNet Labels\n",
      "License: CC-BY-4.0\n",
      "DOI: 10.34911/rdnt.d2ce8i\n",
      "Citation: Alemohammad S.H., Ballantyne A., Bromberg Gaber Y., Booth K., Nakanuku-Diggs L., & Miglarese A.H. (2020) \"LandCoverNet: A Global Land Cover Classification Training Dataset\", Version 1.0, Radiant MLHub. [Date Accessed] https://doi.org/10.34911/rdnt.d2ce8i\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(f'{API_BASE}/collections/{COLLECTION_ID}', headers=headers)\n",
    "print(f'Description: {r.json()[\"description\"]}')\n",
    "print(f'License: {r.json()[\"license\"]}')\n",
    "print(f'DOI: {r.json()[\"sci:doi\"]}')\n",
    "print(f'Citation: {r.json()[\"sci:citation\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Possible Land Cover Labels\n",
    "====\n",
    "\n",
    "Each label item within the collection has a property which lists all of the possible land cover types and which ones are present in each label item. The code below prints out which land cover types are present in the dataset and we will reference these later in the notebook when we filter downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes for labels\n",
      "- (Semi) Natural Vegetation\n",
      "- Artificial Bareground\n",
      "- Cultivated Vegetation\n",
      "- Natural Bareground\n",
      "- No Data\n",
      "- Permanent Snow/Ice\n",
      "- Water\n",
      "- Woody Vegetation\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(f'{API_BASE}/collections/{COLLECTION_ID}/items', headers=headers)\n",
    "label_classes = r.json()['features'][0]['properties']['label:classes']\n",
    "for label_class in label_classes:\n",
    "    print(f'Classes for {label_class[\"name\"]}')\n",
    "    for c in sorted(label_class['classes']):\n",
    "        print(f'- {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Items\n",
    "====\n",
    "\n",
    "The code below sets up functions which we will use to download items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # Required to download assets hosted on S3\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import arrow\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def download_s3(uri, path):\n",
    "    parsed = urlparse(uri)\n",
    "    bucket = parsed.netloc\n",
    "    key = parsed.path[1:]\n",
    "    s3.download_file(bucket, key, os.path.join(path, key.split('/')[-1]))\n",
    "    \n",
    "def download_http(uri, path):\n",
    "    parsed = urlparse(uri)\n",
    "    r = requests.get(uri)\n",
    "    f = open(os.path.join(path, parsed.path[1:]), 'wb')\n",
    "    for chunk in r.iter_content(chunk_size=512 * 1024): \n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "    f.close()\n",
    "\n",
    "def get_download_uri(uri):\n",
    "    r = requests.get(uri, allow_redirects=False)\n",
    "    return r.headers['Location']\n",
    "\n",
    "def download(d):\n",
    "    href = d[0]\n",
    "    path = d[1]\n",
    "    download_uri = get_download_uri(href)\n",
    "    parsed = urlparse(download_uri)\n",
    "    \n",
    "    if parsed.scheme in ['s3']:\n",
    "        download_s3(download_uri, path)\n",
    "    elif parsed.scheme in ['http', 'https']:\n",
    "        download_http(download_uri, path)\n",
    "        \n",
    "def get_source_item_assets(args):\n",
    "    path = args[0]\n",
    "    href = args[1]\n",
    "    asset_downloads = []\n",
    "    try:\n",
    "        r = requests.get(href, headers=headers)\n",
    "    except:\n",
    "        print('ERROR: Could Not Load', href)\n",
    "        return []\n",
    "    dt = arrow.get(r.json()['properties']['datetime']).format('YYYY_MM_DD')\n",
    "    asset_path = os.path.join(path, dt)\n",
    "    if not os.path.exists(asset_path):\n",
    "        os.makedirs(asset_path)\n",
    "\n",
    "    for key, asset in r.json()['assets'].items():\n",
    "        asset_downloads.append((asset['href'], asset_path))\n",
    "        \n",
    "    return asset_downloads\n",
    "\n",
    "def download_source_and_labels(item):\n",
    "    labels = item.get('assets').get('labels')\n",
    "    links = item.get('links')\n",
    "    \n",
    "    # Make the directory to download the files to\n",
    "    path = f'landcovernet/{item[\"id\"]}/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    source_items = []\n",
    "    \n",
    "    # Download the source imagery\n",
    "    for link in links:\n",
    "        if link['rel'] != 'source':\n",
    "            continue\n",
    "        source_items.append((path, link['href']))\n",
    "        \n",
    "    results = p.map(get_source_item_assets, source_items)\n",
    "    results.append([(labels['href'], path)])\n",
    "            \n",
    "    return results\n",
    "\n",
    "def get_items(uri, classes=None, max_items_downloaded=None, items_downloaded=0, downloads=[]):\n",
    "    print('Loading', uri, '...')\n",
    "    r = requests.get(uri, headers=headers)\n",
    "    collection = r.json()\n",
    "    for feature in collection.get('features', []):\n",
    "        # Check if the item has one of the label classes we're interested in\n",
    "        matches_class = True\n",
    "        if classes is not None:\n",
    "            matches_class = False\n",
    "            for label_class in feature['properties'].get('labels', []):\n",
    "                if label_class in classes:\n",
    "                    matches_class = True\n",
    "                    break\n",
    "            \n",
    "        # If the item does not match all of the criteria we specify, skip it\n",
    "        if not matches_class:\n",
    "            continue\n",
    "        \n",
    "        print('Getting Source Imagery Assets for', feature['id'])\n",
    "        # Download the label and source imagery for the item\n",
    "        downloads.extend(download_source_and_labels(feature))\n",
    "        \n",
    "        # Stop downloaded items if we reached the maximum we specify\n",
    "        items_downloaded += 1\n",
    "        if max_items_downloaded is not None and items_downloaded >= max_items_downloaded:\n",
    "            return downloads\n",
    "        \n",
    "    # Get the next page if results, if available\n",
    "    for link in collection.get('links', []):\n",
    "        if link['rel'] == 'next' and link['href'] is not None:\n",
    "            get_items(link['href'], classes=classes, max_items_downloaded=max_items_downloaded, items_downloaded=items_downloaded, downloads=downloads)\n",
    "    \n",
    "    return downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading 10 Items\n",
    "====\n",
    "\n",
    "The function below will navigate and API and collect all the download links for labels and source imagery assets. In this function we specified the \"max_items_downloaded\" argument which limits the number of label items downloaded. By removing this argument you can download all of the label items which match the criteria you specify. After collecting the download links we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(20)\n",
    "to_download = get_items(f'{API_BASE}/collections/{COLLECTION_ID}/items?limit=100',\n",
    "                        max_items_downloaded=10, downloads=[])\n",
    "for d in tqdm(to_download):\n",
    "    p.map(download, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering on Land Cover Type\n",
    "====\n",
    "\n",
    "We can specify which land cover types we want to download by adding the \"classes\" argument. This argument accepts an array of land cover types and only label items which contain one or more of the classes specified will be downloaded. The possible land cover types can be found in the \"Finding Possible Land Cover Labels\" cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = get_items(f'{API_BASE}/collections/{COLLECTION_ID}/items?limit=100',\n",
    "                        classes=['Woody Vegetation'],\n",
    "                        max_items_downloaded=10,\n",
    "                        downloads=[])\n",
    "for d in tqdm(to_download):\n",
    "    p.map(download, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading All Items\n",
    "====\n",
    "\n",
    "By removing both the \"classes\" and \"max_items_downloaded\" arguments we can download all of the labels and source imagery in the LandCoverNet dataset. WARNING: This will take a long time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = get_items(f'{API_BASE}/collections/{COLLECTION_ID}/items?limit=100', downloads=[])\n",
    "print('Downloading Assets')\n",
    "for d in tqdm(to_download):\n",
    "    p.map(download, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
